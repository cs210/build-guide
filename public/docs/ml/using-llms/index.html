<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Using LLMs
  #

Below are some ways to go beyond just LLMs and use your custom data. These include methods such as Retrieval Augmented Generation (RAG), finetuning, and even searching over tabular data.

  Prompt Engineering
  #

LLMs are trained on a ton of text, so we can often get a lot out of them just by clever prompting. Finding the best way to prompt a model is also known as prompt engineering, and there are several popular methods, including chain-of-thought (including the model&rsquo;s reasoning step-by-step) as well as few-shot prompting (providing a few sample examples)."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/ml/using-llms/"><meta property="og:site_name" content="CS 210 Build Guide"><meta property="og:title" content="Using LLMs"><meta property="og:description" content="Using LLMs # Below are some ways to go beyond just LLMs and use your custom data. These include methods such as Retrieval Augmented Generation (RAG), finetuning, and even searching over tabular data.
Prompt Engineering # LLMs are trained on a ton of text, so we can often get a lot out of them just by clever prompting. Finding the best way to prompt a model is also known as prompt engineering, and there are several popular methods, including chain-of-thought (including the model’s reasoning step-by-step) as well as few-shot prompting (providing a few sample examples)."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:published_time" content="2025-01-08T10:55:07-08:00"><meta property="article:modified_time" content="2025-01-08T10:55:07-08:00"><title>Using LLMs | CS 210 Build Guide</title>
<link rel=icon href=../../../favicon.png><link rel=manifest href=../../../manifest.json><link rel=canonical href=http://localhost:1313/docs/ml/using-llms/><link rel=stylesheet href=../../../book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=../../../fuse.min.js></script><script defer src=../../../en.search.min.9adaf7cc0f5ff39ad039619268023ed7af837433d1a65d72e4c6251ca86f6ce4.js integrity="sha256-mtr3zA9f85rQOWGSaAI+16+DdDPRpl1y5MYlHKhvbOQ=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=../../../><span>CS 210 Build Guide</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=../../../docs/class-resources/>Class Resources</a></li><li><span>Web</span><ul><li><a href=../../../docs/web/basic-website-setup/>Basic Website Setup</a></li><li><a href=../../../docs/web/frontend-essentials/>Frontend Essentials</a></li><li><a href=../../../docs/web/databases/>Databases</a></li><li><a href=../../../docs/web/analytics/>Analytics</a></li></ul></li><li><span>ML Engineering</span><ul><li><a href=../../../docs/ml/basic-ml/>Basic ML</a></li><li><a href=../../../docs/ml/using-llms/ class=active>Using LLMs</a></li><li><a href=../../../docs/ml/potpourri-resources/>A Potpourri of Resources</a></li></ul></li><li><span>AR/VR</span><ul><li><a href=../../../docs/vr/getting-started-in-vr/>Getting Started in VR</a></li></ul></li><li><span>Mobile</span><ul><li><a href=../../../docs/mobile/mobile-basics/>Mobile Basics</a></li></ul></li><li><span>Company Building</span><ul><li><a href=../../../docs/cb/talking-to-users/>Talking to Users</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=../../../svg/menu.svg class=book-icon alt=Menu></label><h3>Using LLMs</h3><label for=toc-control><img src=../../../svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#prompt-engineering>Prompt Engineering</a></li><li><a href=#retrieval-augmented-generation>Retrieval Augmented Generation</a></li><li><a href=#finetuning-language-models>Finetuning Language Models</a></li><li><a href=#function-callingtool-use>Function Calling/Tool Use</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=using-llms>Using LLMs
<a class=anchor href=#using-llms>#</a></h1><p>Below are some ways to go beyond just LLMs and use your custom data. These include methods such as Retrieval Augmented Generation (RAG), finetuning, and even searching over tabular data.</p><h2 id=prompt-engineering>Prompt Engineering
<a class=anchor href=#prompt-engineering>#</a></h2><p>LLMs are trained on a ton of text, so we can often get a lot out of them just by clever prompting. Finding the best way to prompt a model is also known as <a href=https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/><strong>prompt engineering</strong></a>, and there are several popular methods, including chain-of-thought (including the model&rsquo;s reasoning step-by-step) as well as few-shot prompting (providing a few sample examples).</p><h2 id=retrieval-augmented-generation>Retrieval Augmented Generation
<a class=anchor href=#retrieval-augmented-generation>#</a></h2><p>Language models are typically pretrained on vast amounts of data, but they may require updated information or data tailored to a specific use case. Retraining a language model on a large amount of data might prove to be expensive. In many scenarios, extending large language models with external knowledge bases becomes essential to provide relevant and accurate outputs. This is where <strong>Retrieval-Augmented Generation (RAG)</strong> comes into play. RAG consists of two key components: a retrieval system to identify relevant items or contextual information and a mechanism to integrate that information into the model&rsquo;s output, often through prompting.</p><p><img src=https://python.langchain.com/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png alt="Retrieval-Augmented Generation Concepts"></p><p>To implement a knowledge base for RAG, you often use an embedding model, which encodes information into a lower-dimensional space, and a vector database, which performs similarity searches within that space. Tools like <a href=https://www.trychroma.com/>Chroma</a>, <a href=https://www.pinecone.io/>Pinecone</a>, and <a href=https://github.com/pgvector/pgvector>pgvector</a> are popular options for this purpose. These systems enable efficient retrieval of contextually relevant data to enhance the model&rsquo;s responses. Below is a simple code example demonstrating this process, and you can find several excellent tutorials for further exploration.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Make sure to pip install chromadb openai </span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> chromadb
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> chromadb.utils <span style=color:#f92672>import</span> embedding_functions
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> chromadb.config <span style=color:#f92672>import</span> Settings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 1: Initialize the Chroma client</span>
</span></span><span style=display:flex><span>chroma_client <span style=color:#f92672>=</span> chromadb<span style=color:#f92672>.</span>Client(
</span></span><span style=display:flex><span>    Settings(
</span></span><span style=display:flex><span>        persist_directory<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;./chroma_db&#34;</span>,  <span style=color:#75715e># Path to store the database</span>
</span></span><span style=display:flex><span>        chroma_db_impl<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;duckdb+parquet&#34;</span>,  <span style=color:#75715e># Database backend</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 2: Define the embedding function (OpenAI in this case)</span>
</span></span><span style=display:flex><span>openai_ef <span style=color:#f92672>=</span> embedding_functions<span style=color:#f92672>.</span>OpenAIEmbeddingFunction(
</span></span><span style=display:flex><span>    api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;your_openai_api_key&#34;</span>,  <span style=color:#75715e># Replace with your OpenAI API key</span>
</span></span><span style=display:flex><span>    model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;text-embedding-ada-002&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 3: Create or get a collection</span>
</span></span><span style=display:flex><span>collection <span style=color:#f92672>=</span> chroma_client<span style=color:#f92672>.</span>get_or_create_collection(
</span></span><span style=display:flex><span>    name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;knowledge_base&#34;</span>, embedding_function<span style=color:#f92672>=</span>openai_ef
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 4: Add data to the collection</span>
</span></span><span style=display:flex><span>documents <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;The Eiffel Tower is located in Paris, France.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;The Great Wall of China is one of the Seven Wonders of the World.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Python is a popular programming language for machine learning.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;The Amazon Rainforest is the largest rainforest in the world.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Albert Einstein developed the theory of relativity.&#34;</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>doc_ids <span style=color:#f92672>=</span> [<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;doc_</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(documents))]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>collection<span style=color:#f92672>.</span>add(
</span></span><span style=display:flex><span>    documents<span style=color:#f92672>=</span>documents,
</span></span><span style=display:flex><span>    ids<span style=color:#f92672>=</span>doc_ids
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 5: Query the collection with a similar question</span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Who created the theory of relativity?&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> collection<span style=color:#f92672>.</span>query(
</span></span><span style=display:flex><span>    query_texts<span style=color:#f92672>=</span>[query],
</span></span><span style=display:flex><span>    n_results<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>  <span style=color:#75715e># Return top 3 most similar results</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 6: Display results</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Query:&#34;</span>, query)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Most similar documents:&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, doc <span style=color:#f92672>in</span> enumerate(results[<span style=color:#e6db74>&#34;documents&#34;</span>][<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>. </span><span style=color:#e6db74>{</span>doc<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>Using these documents, we can then use them when prompting a final language model to improve overall responses.</p><h2 id=finetuning-language-models>Finetuning Language Models
<a class=anchor href=#finetuning-language-models>#</a></h2><p>Sometimes, prompting isn&rsquo;t enough. You may want to adjust and steer the tone of a large language model (LLM) or train it on additional examples that exceed the limits of a single prompt. To achieve this, <strong>fine-tuning</strong> is a powerful tool.</p><p>Fine-tuning is the process of adapting a pre-trained model for specific tasks or use cases. It is significantly easier and more cost-effective to improve a base model through fine-tuning than to train a model from scratch.</p><p>During rapid prototyping, you&rsquo;ll likely rely on a pre-trained LLM. If needed, there are well-documented ways to fine-tune pre-trained LLMs (<a href=https://platform.openai.com/docs/guides/fine-tuning>OpenAI Fine-Tuning Guide</a>). The most important steps in fine-tuning include:</p><ul><li><strong>Preparing your data</strong>: Ensure your dataset is in the correct format, and include both positive and negative examples so the LLM can learn to handle a variety of scenarios effectively.</li><li><strong>Choosing the right dataset size</strong>: A minimum of 10 examples can be sufficient, but you’ll observe clearer improvements with 50–100 examples or more.</li><li><strong>Setting up evaluations</strong>: Create a set of prompts that can be used to test and measure the fine-tuned model’s performance against your desired outcomes.</li></ul><p>Once the data and evaluations are ready, you can initiate a fine-tuning job using OpenAI&rsquo;s API:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> openai <span style=color:#f92672>import</span> OpenAI
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> OpenAI()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>job <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>fine_tuning<span style=color:#f92672>.</span>jobs<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    training_file<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;file-all-about-the-weather&#34;</span>,
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gpt-4o-2024-08-06&#34;</span>,
</span></span><span style=display:flex><span>    method<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;dpo&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;dpo&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;hyperparameters&#34;</span>: {<span style=color:#e6db74>&#34;beta&#34;</span>: <span style=color:#ae81ff>0.1</span>},
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>After fine-tuning, you can use the customized model in your application:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> openai <span style=color:#f92672>import</span> OpenAI
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> OpenAI()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>completion <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>chat<span style=color:#f92672>.</span>completions<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ft:gpt-4o-mini:my-org:custom_suffix:id&#34;</span>,
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;You are a helpful assistant.&#34;</span>},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Hello!&#34;</span>}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(completion<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>message)
</span></span></code></pre></div><p>You can experiment with different hyperparameters to improve the performance of your fine-tuned model. Fine-tuning an open-source model is also a viable option. There are excellent resources available, such as the <a href=https://www.llama.com/docs/how-to-guides/fine-tuning/>Llama Fine-Tuning Guide</a>. Additionally, there are many techniques (<a href=https://www.ibm.com/think/topics/fine-tuning>IBM Fine-Tuning Techniques</a>) to make the process more cost-effective and computationally efficient.</p><h2 id=function-callingtool-use>Function Calling/Tool Use
<a class=anchor href=#function-callingtool-use>#</a></h2><p>We showed above how we might incorporate external data in our LLM pipelines. However, we may also need for our model to interact with other services, such as APIs and UIs. In this case, we&rsquo;ll lean on <strong>function calling, or tool use</strong>.</p><p>The below diagram gives a good overview of how to set up function calling, with many of the popular pre-trained LLMs already having <a href=https://platform.openai.com/docs/guides/function-calling>infrastructure</a> to enable this. In general, you will need to: define the tools to the LLM, determine when to call the tool, and use the results downstream.</p><p><img src=https://cdn.openai.com/API/docs/images/function-calling-diagram-steps.png alt="Function Calling Pipeline"></p><p>As a quick example, let&rsquo;s first implement a function that gets the current weather using an API call:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_weather</span>(latitude, longitude):
</span></span><span style=display:flex><span>    response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;https://api.open-meteo.com/v1/forecast?latitude=</span><span style=color:#e6db74>{</span>latitude<span style=color:#e6db74>}</span><span style=color:#e6db74>&amp;longitude=</span><span style=color:#e6db74>{</span>longitude<span style=color:#e6db74>}</span><span style=color:#e6db74>&amp;current=temperature_2m,wind_speed_10m&amp;hourly=temperature_2m,relative_humidity_2m,wind_speed_10m&#34;</span>)
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> response<span style=color:#f92672>.</span>json()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> data[<span style=color:#e6db74>&#39;current&#39;</span>][<span style=color:#e6db74>&#39;temperature_2m&#39;</span>]
</span></span></code></pre></div><p>Using the <code>openai</code> SDK, we can then define this tool for our language model to use:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> openai <span style=color:#f92672>import</span> OpenAI
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> OpenAI()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tools <span style=color:#f92672>=</span> [{
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;function&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;function&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;get_weather&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;description&#34;</span>: <span style=color:#e6db74>&#34;Get current temperature for provided coordinates in celsius.&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;parameters&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;object&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;properties&#34;</span>: {
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;latitude&#34;</span>: {<span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;number&#34;</span>},
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;longitude&#34;</span>: {<span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;number&#34;</span>}
</span></span><span style=display:flex><span>            },
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;required&#34;</span>: [<span style=color:#e6db74>&#34;latitude&#34;</span>, <span style=color:#e6db74>&#34;longitude&#34;</span>],
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;additionalProperties&#34;</span>: <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;strict&#34;</span>: <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>messages <span style=color:#f92672>=</span> [{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;What&#39;s the weather like in Paris today?&#34;</span>}]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>completion <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>chat<span style=color:#f92672>.</span>completions<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gpt-4o&#34;</span>,
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>messages,
</span></span><span style=display:flex><span>    tools<span style=color:#f92672>=</span>tools,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>If the model decides to use the tool, it&rsquo;ll respond to the message with a request to get the weather:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[{
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;id&#34;</span>: <span style=color:#e6db74>&#34;call_12345xyz&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;function&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;function&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;get_weather&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;arguments&#34;</span>: <span style=color:#e6db74>&#34;{</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>latitude</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>:48.8566,</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>longitude</span><span style=color:#ae81ff>\&#34;</span><span style=color:#e6db74>:2.3522}&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}]
</span></span></code></pre></div><p>We can then execute the function code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tool_call <span style=color:#f92672>=</span> completion<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>message<span style=color:#f92672>.</span>tool_calls[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>args <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(tool_call<span style=color:#f92672>.</span>function<span style=color:#f92672>.</span>arguments)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>result <span style=color:#f92672>=</span> get_weather(args[<span style=color:#e6db74>&#34;latitude&#34;</span>], args[<span style=color:#e6db74>&#34;longitude&#34;</span>])
</span></span></code></pre></div><p>And then supply the result to the model and call it again.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>messages<span style=color:#f92672>.</span>append(completion<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>message)  <span style=color:#75715e># append model&#39;s function call message</span>
</span></span><span style=display:flex><span>messages<span style=color:#f92672>.</span>append({                               <span style=color:#75715e># append result message</span>
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;tool&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;tool_call_id&#34;</span>: tool_call<span style=color:#f92672>.</span>id,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;content&#34;</span>: result
</span></span><span style=display:flex><span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>completion_2 <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>chat<span style=color:#f92672>.</span>completions<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gpt-4o&#34;</span>,
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>messages,
</span></span><span style=display:flex><span>    tools<span style=color:#f92672>=</span>tools,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Overall, function calling is how we enable the development of <a href=https://huyenchip.com/2025/01/07/agents.html>truly agentic workflows</a>.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#prompt-engineering>Prompt Engineering</a></li><li><a href=#retrieval-augmented-generation>Retrieval Augmented Generation</a></li><li><a href=#finetuning-language-models>Finetuning Language Models</a></li><li><a href=#function-callingtool-use>Function Calling/Tool Use</a></li></ul></nav></div></aside></main></body></html>