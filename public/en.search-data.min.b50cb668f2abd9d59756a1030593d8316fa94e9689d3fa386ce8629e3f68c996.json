[{"id":0,"href":"/docs/ml/basic/","title":"Basic ML","section":"ML Engineering","content":" Basic ML # Below are some snippets for common ML use cases such as NLP \u0026ndash; namely, LLMs \u0026ndash; computer vision, and even standard statistical ML.\nLarge Language Models (LLMs) # For initial iteration, using LLM APIs is probably the best way to go. There are several models out there, each with their own strengths. Below is an example using the OpenAI API to make a call to a language model:\nfrom openai import OpenAI client = OpenAI() completion = client.chat.completions.create( model=\u0026#34;gpt-4o-mini\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Write a haiku about recursion in programming.\u0026#34; } ] ) print(completion.choices[0].message) As you continue to iterate, you may want to leverage the power of open-source models. For this, Hugging Face \u0026ndash; in particular, the SentenceTransformers module \u0026ndash; will be your best friend. Just specify your language model and you\u0026rsquo;ll be good to go:\nfrom sentence_transformers import SentenceTransformer model = SentenceTransformer(\u0026#39;paraphrase-MiniLM-L6-v2\u0026#39;) # Sentences we want to encode. Example: sentence = [\u0026#39;This framework generates embeddings for each input sentence\u0026#39;] # Sentences are encoded by calling model.encode() embedding = model.encode(sentence) Of course, utilizing an open-source model will also mean figuring out how to serve the model. There are a plethora of tools for this, including Modal, Baseten, Together AI, and many others.\nComputer Vision # Similar to LLMs, there are several Vision APIs available that can be used for quick experimentation. All of the larger cloud platforms have existing offerings \u0026ndash; namely GCP and Azure \u0026ndash; but it\u0026rsquo;s also important to note that many of the SOTA models are already multimodal. For instance, GPT-4V already has strong vision capabilities:\nfrom openai import OpenAI client = OpenAI() response = client.chat.completions.create( model=\u0026#34;gpt-4o-mini\u0026#34;, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;What\u0026#39;s in this image?\u0026#34;}, { \u0026#34;type\u0026#34;: \u0026#34;image_url\u0026#34;, \u0026#34;image_url\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\u0026#34;, }, }, ], } ], max_tokens=300, ) print(response.choices[0]) For using custom models, torchvision \u0026ndash; and PyTorch more broadly \u0026ndash; are quite helpful. These packages allow us to both build our own models from scratch as well as use existing large pre-trained models. we can both An often common use case is to take a pre-trained image classification or segmentation model and further finetuning it for a specific task. This is also known as transfer learning, and there are several tutorials out there for the task.\nStatistical ML # Finally, for simple problems, statistical techniques may be sufficient. sklearn is often what\u0026rsquo;s used in industry for these tasks, and it can support tasks ranging from nearest neighbors clustering to gradient boosting. Here\u0026rsquo;s an example of using sklearn for linear regression:\nfrom sklearn.linear_model import LinearRegression import numpy as np # Create some sample data X = np.array([[1], [2], [3], [4], [5]]) # Features y = np.array([2, 4, 5, 4, 5]) # Target values # Create and fit the model model = LinearRegression() model.fit(X, y) # Make predictions predictions = model.predict(X) # Get model coefficients and intercept print(f\u0026#34;Slope: {model.coef_[0]:.2f}\u0026#34;) print(f\u0026#34;Intercept: {model.intercept_:.2f}\u0026#34;) # Calculate R-squared score r_squared = model.score(X, y) print(f\u0026#34;R-squared: {r_squared:.2f}\u0026#34;) "},{"id":1,"href":"/docs/class-resources/","title":"Class Resources","section":"Docs","content":" Class Resources # Below are some general class resources, ranging from deployment credits to general tools of the trade.\nVercel Credits # Students can create a Vercel Hobby account as soon as the class starts - Hobby accounts are always free If they\u0026rsquo;d like to explore Pro features, we\u0026rsquo;ve set up \u0026ldquo;Stanford CS\u0026rdquo; as a category in the Vercel Credits for Startups application page. Each team will receive $1,200 in credits. For \u0026ldquo;Proof of Partnership\u0026rdquo;, they should upload a screenshot of the syllabus / an e-mail from Jay about the course. A 14-day Pro Trial begins (they can email josh.oynick@vercel.com with any issues). At the end of the trial, if the teams choose to remain on Pro and add credit card info, the credits will begin to draw down. If teams add payment info, they are strongly encouraged to set hard spending limits right away (documentation). Tools of the Trade # Documentation # Github Wiki Here are some examples Project Management # Github issues, milestones, labels, comments Here is an example Real-time Team Communication # Slack, Messenger Discovery, Concepting Phase # Hand draw, Miro, Figma, GSlides Build Phase # Github Code (Pull request methodology) Github issues (code focused) Leverage Copilot, chatGPT, Bard as is useful GitHub Resources # Make sure your git activity is attributed to you Github Cheat Sheet - Page 1 Github Cheat Sheet - Page 2 How to make a Github pull request Miscellany Resources # Messaging Activation Matrix Beginner\u0026rsquo;s resources for Building Web and mobile apps CS210 Recommended Reads for Aspiring Program Managers/Product Leaders Contract of Deliverables Template Product Requirements Document (PRD) An incomplete list of ethics questions to consider Team Budget Guidelines Grading Rubric Make an animated gif from screen shots Digital sticky note option "},{"id":2,"href":"/docs/ml/extending-llms/","title":"Extending LLMs","section":"ML Engineering","content":" Extending LLMs # Below are some ways to go beyond just LLMs and use your custom data. These include methods such as finetuning, RAG, and even searching over tabular data.\nFinetuning # RAG # Tabular Data # "},{"id":3,"href":"/docs/ml/general-advice/","title":"General Advice","section":"ML Engineering","content":" General Advice # Below are some general tips, aggregated from industry experts and 210 alums.\nBuy first, not Build. # For initial functional prototyping and for most of the former part of your application\u0026rsquo;s lifecycle, lean on existing APIs to do a lot of the heavy lifting. These models are already quite powerful and are able to handle almost all of the requests you send it. If certain users or design partners necessitate even more advanced functionality, though, consider working and finetuning your own LLMs.\nWhen unsure, lean on the literature. # Oftentimes, issues with performance lie in the technical reports.\n"},{"id":4,"href":"/docs/vr/getting-started-in-vr/","title":"Getting Started in VR","section":"AR/VR","content":" Getting Started in VR # Original document by Mindy:\nLast year I did a VR project in CS210 with Unreal Engine, and thought that I would share some thoughts and tips with you guys to get you started.\nDemos # Title Link Description Oculus Demos repository https://share.oculus.com/ The definitive VR experiences and demos repository from Oculus. Elite Dangerous https://www.elitedangerous.com/ A space exploration video game with a great VR experience. Costs $45. Things to think about # what sort of approaches to games/experiences might work for your project? What are some common/different control interfaces/menus/UI that the demos use? What works better or worse? what hasn\u0026rsquo;t been done/done well? What can you bring to the table? There are also loads of other VR demos available - if you want to see if a kind of demo exists just Google it and you\u0026rsquo;ll probably turn something up. Also check out Cardboard demos on Google Play (I recommend VRSE) to see what sort of things are happening in the non-gaming/low-end space. VR best practices # Google - http://www.fastcodesign.com/3053288/3-tips-on-designing-for-vr-from-google, https://play.google.com/store/apps/details?id=com.google.vr.cardboard.apps.designlab Unreal - https://docs.unrealengine.com/latest/INT/Platforms/VR/ContentSetup/index.html Oculus - https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/ Search for \u0026ldquo;VR best practices for [insert company/device/controller]\u0026rdquo; and you\u0026rsquo;ll probably unearth a ton of stuff Unreal Engine tutorials # Unreal has a really good YouTube series where a developer sits there and shows you how to make a demo. This was by far the most helpful way to get me oriented. https://www.youtube.com/playlist?list=PLZlv_N0_O1gak1_FoAJVrEGiLIploeF3F LayoutVR has good walkthrough guides You can go read the official Unreal Engine guides too Unreal has a steep learning curve if you\u0026rsquo;ve never done game dev before, so make sure you allocate a lot of time to learning it! Controllers # This is still a huge pain point in VR, and likely the bottleneck to what you can accomplish. Oculus Touch is probably your best bet if you can get your hands on it Razer Hydra/Sixense STEM or the Kinect are other good But really, try to find all your possible options. If the controller is not exactly commercially available yet, don\u0026rsquo;t be afraid to ask for a prototype. You won\u0026rsquo;t believe how willing people are to work with you once you drop the Stanford/Unreal/Facebook/Oculus name. Mindy\u0026rsquo;s team\u0026rsquo;s work from last year # https://unrealvr.wordpress.com/ I\u0026rsquo;ve also done stuff on the Cardboard and some product research on the space in general. Feel free to ask me any questions! There is a LOT of information to just absorb if you\u0026rsquo;re completely new to the space.\nFeeling a bit overwhelmed is normal, just be tenacious, ask tons of questions and keep on reading!\n"}]