[{"id":0,"href":"/docs/mobile/mobilebasics/","title":"Mobile Basics","section":"Mobile","content":" Mobile Basics # Language choice # For building a mobile app we would suggest using React Native. It renders native mobile componenets and almost all code can be shared between IOS and Android.\nIf you want higher performance on a specific type of device it might be worth considering the use of a Swift(IOS) or Kotlin(Android).\nDeveloping and Running Apps # We suggest that you use expo to test your app throughout your development process. It\u0026rsquo;s accessible and has everything your team will need most of the time. You can also write for both iOS and Android regardless of your development machine.\nIf you need maximum flexibility, want to use or write specific, native modules, and/or have prior React Native and app development experience, consider React Native CLI.\nReact Native Resources # For a simple step-by-step setup of React Native and Expo check out this 10 minute video.\nFor comprehensive learning resources check out the course resources for CS 147L.\nFor in depth explanation check out the React-Native documentation.\nReact Native with Expo - Development Guide # Development Enviroment Setup # Setup and Installation # First, check if you have Node installed. If not, download Node:\nnode -v Then follow these commands:\n# Install Expo CLI globally - this is like installing your main toolkit npm install -g expo-cli # Create a new Expo project - this creates a new folder with all the basic files you need npx create-expo-app MyCapstoneProject # Start development - this will open a development server cd MyCapstoneProject npx expo start After running npx expo start, you\u0026rsquo;ll see a QR code. Install the Expo Go app on your phone, scan this QR code, and you\u0026rsquo;ll see your app running on your device!\nProject Structure # Starting off with good organization makes future development easier. Here is a good starting folder structure:\nMyCapstoneProject/ ├── assets/ # Your app\u0026#39;s images, fonts, and other files ├── components/ # Reusable pieces of your app\u0026#39;s interface ├── screens/ # Full pages of your app ├── navigation/ # Code that handles moving between screens ├── hooks/ # Reusable logic ├── App.js # The main file that starts your app └── app.json # Settings for your app Essential Components and Patterns # 1. Creating Your First Screen # A screen is like a full page in your app. Here\u0026rsquo;s a basic example with explanations:\n// screens/HomeScreen.js import { View, Text, StyleSheet } from \u0026#39;react-native\u0026#39;; import { StatusBar } from \u0026#39;expo-status-bar\u0026#39;; // This is your main screen component const HomeScreen = () =\u0026gt; { return ( // View is like a \u0026lt;div\u0026gt; in web development - it\u0026#39;s a container \u0026lt;View style={styles.container}\u0026gt; {/* StatusBar controls the top bar of the phone */} \u0026lt;StatusBar style=\u0026#34;auto\u0026#34; /\u0026gt; {/* Text is used for any text you want to display */} \u0026lt;Text style={styles.title}\u0026gt;Welcome!\u0026lt;/Text\u0026gt; \u0026lt;/View\u0026gt; ); }; // Styles in React Native are similar to CSS but with camelCase names // They\u0026#39;re defined using StyleSheet.create for better performance const styles = StyleSheet.create({ container: { flex: 1, // This makes the container take up all available space padding: 16, alignItems: \u0026#39;center\u0026#39;, // Center items horizontally justifyContent: \u0026#39;center\u0026#39;, // Center items vertically }, title: { fontSize: 24, fontWeight: \u0026#39;bold\u0026#39;, }, }); export default HomeScreen; 2. Navigation (Moving Between Screens) # Navigation is how users move between different screens in your app. Think of it like changing pages in a book:\n// navigation/AppNavigator.js import { NavigationContainer } from \u0026#39;@react-navigation/native\u0026#39;; import { createNativeStackNavigator } from \u0026#39;@react-navigation/native-stack\u0026#39;; // Stack navigation works like a stack of cards // You can push new screens on top and pop them off to go back const Stack = createNativeStackNavigator(); const AppNavigator = () =\u0026gt; { return ( \u0026lt;NavigationContainer\u0026gt; \u0026lt;Stack.Navigator\u0026gt; {/* Each Screen component is like a page in your app */} \u0026lt;Stack.Screen name=\u0026#34;Home\u0026#34; component={HomeScreen} options={{ title: \u0026#39;Home\u0026#39;, // This appears at the top of the screen headerStyle: { backgroundColor: \u0026#39;#f4511e\u0026#39; }, // Customize the header headerTintColor: \u0026#39;#fff\u0026#39;, // Color of the header text }} /\u0026gt; \u0026lt;/Stack.Navigator\u0026gt; \u0026lt;/NavigationContainer\u0026gt; ); }; 3. Working with Data (API Calls) # Most apps need to fetch data from the internet. Here\u0026rsquo;s a reusable way to do that:\n// hooks/useApi.js import { useState, useEffect } from \u0026#39;react\u0026#39;; // This is a custom hook - a reusable piece of logic const useApi = (url) =\u0026gt; { // useState is used to store and update data const [data, setData] = useState(null); const [loading, setLoading] = useState(true); const [error, setError] = useState(null); // useEffect runs when the component is first shown useEffect(() =\u0026gt; { fetchData(); }, [url]); // This function gets data from the internet const fetchData = async () =\u0026gt; { try { const response = await fetch(url); const json = await response.json(); setData(json); } catch (err) { setError(err.message); } finally { setLoading(false); } }; return { data, loading, error }; }; // Example of using this hook in a component const MyComponent = () =\u0026gt; { const { data, loading, error } = useApi(\u0026#39;https://api.example.com/data\u0026#39;); // Show a loading spinner while waiting for data if (loading) return \u0026lt;ActivityIndicator /\u0026gt;; // Show an error message if something went wrong if (error) return \u0026lt;Text\u0026gt;Error: {error}\u0026lt;/Text\u0026gt;; // Show the data once it\u0026#39;s loaded return \u0026lt;Text\u0026gt;{data.title}\u0026lt;/Text\u0026gt;; }; 4. Working with Images # Images are a crucial part of most apps. Here\u0026rsquo;s how to handle them in Expo:\n// components/ProfileImage.js import { Image } from \u0026#39;react-native\u0026#39;; import { Asset } from \u0026#39;expo-asset\u0026#39;; const ProfileImage = ({ uri }) =\u0026gt; { return ( // uri is the internet address of the image // The placeholder image shows while the main image is loading \u0026lt;Image source={{ uri }} style={{ width: 100, height: 100, borderRadius: 50 }} defaultSource={require(\u0026#39;../assets/placeholder.png\u0026#39;)} /\u0026gt; ); }; 5. Storing Data Locally # Sometimes you need to save data on the device (like user settings or login information):\n// utils/storage.js import * as SecureStore from \u0026#39;expo-secure-store\u0026#39;; // Save data securely on the device export const saveData = async (key, value) =\u0026gt; { try { await SecureStore.setItemAsync(key, JSON.stringify(value)); } catch (error) { console.error(\u0026#39;Error saving data:\u0026#39;, error); } }; // Get data that was previously saved export const getData = async (key) =\u0026gt; { try { const value = await SecureStore.getItemAsync(key); return value ? JSON.parse(value) : null; } catch (error) { console.error(\u0026#39;Error retrieving data:\u0026#39;, error); return null; } }; Best Practices for Beginners # Start Small: Begin with a simple screen and add features gradually Use Console.log: When something\u0026rsquo;s not working, use console.log to print values and understand what\u0026rsquo;s happening Test Frequently: Make small changes and test them right away on your device Keep Components Small: Break your screens into smaller, reusable components Learn from Errors: Read error messages carefully - they often tell you exactly what\u0026rsquo;s wrong Common Issues and Solutions # App Not Updating: Try restarting the Expo server (press \u0026lsquo;r\u0026rsquo; in the terminal) Styling Not Working: Make sure you\u0026rsquo;re using the correct style properties (they\u0026rsquo;re slightly different from CSS) Can\u0026rsquo;t Connect to Device: Ensure your phone and computer are on the same WiFi network Images Not Loading: Double-check the image path and make sure the image exists Testing Your App # The easiest way to test your app is using the Expo Go app on your phone:\nStart your development server (npx expo start) Open Expo Go on your phone Scan the QR code from your terminal The app will load on your device Publishing Your App # When you\u0026rsquo;re ready to share your app:\n# Create a production version expo build:android # For Android expo build:ios # For iOS # Test the production version expo start --no-dev --minify # Share your app through Expo expo publish Getting Help # Check the Expo Documentation Leverage your favorite AI chatbot Look for similar issues on Stack Overflow Join the Expo Discord community Remember: Everyone starts somewhere, and it\u0026rsquo;s okay to make mistakes. The key is to learn from them and keep building!\n"},{"id":1,"href":"/docs/web/starthere/","title":"Start Here","section":"Web","content":" Basic Website Setup # Below is a quick setup guide for building a website with Next.js and Vercel — the perfect infrastructure to add V0 code to!\nProject Setup # First, check if you have Node installed. If not, download Node:\nnode -v Next, create a new web project. The following command includes presets that integrate well with V0 code:\nnpx create-next-app@latest project-name --typescript --eslint --tailwind --app --src-dir --import-alias \u0026#34;@/*\u0026#34; Say no to Turbopack.\nNavigate to the project-name directory:\ncd project-name Recomended Directory Structure # my-app/ ├── app/ # Pages and API routes ├── components/ # Reusable UI components ├── lib/ # Utilities and configurations ├── hooks/ # Custom React hooks ├── types/ # TypeScript definitions └── public/ # Static assets Go Build!! # Go to V0, sign up, and prompt it to build a website of your choosing.\nTry this:\nClick on the Add to Codebase icon, copy the command, and run it in your project root directory (project-name). Install the packages and answer the following questions: Components.js file: Yes Style: Default Color: Neutral CSS Variables: Yes However, there is currently an unresolved bug with the tech that fetches V0 code. If it doesn\u0026rsquo;t work, here is a workaround:\nManually download the files via zip and copy them over. Only move files that have been updated or added such as the components, hooks, lib and style folders. To test your website locally:\nnpm run dev Paste the local host URL printed in your terminal into your browser to test your website.\nIf you get errors, check if you need to download any extra dependencies and make use of your favorite AI chatbot to resolve them.\nPushing to Git # Go to the directory where you want to store all the code for your CS210 project.\nPush your changes to GitHub for the first time:\ngit clone https://github.com/your-username/your-repo.git cd your-repo git add . git commit -m \u0026#34;Add project files\u0026#34; git push Deploying on Vercel # Go to Vercel Import your Git repository Add any environment variables Deploy Ok!! You are now ready to go build the most amazing website of your dreams! Remember to leverage AI tools and dive into documentation when necessary.\nDocumentation # Want to learn Next.js? The following resources are your best bet:\nNext.js Documentation - Learn about Next.js features and APIs. Learn Next.js - An interactive Next.js tutorial. Vercel Templates - Boilerplate code that can be a great starting point or inspiration for a project or component. "},{"id":2,"href":"/docs/class-resources/","title":"Class Resources","section":"Docs","content":" Class Resources # Below are some general class resources, ranging from deployment credits to general tools of the trade.\nVercel Credits # Students can create a Vercel Hobby account as soon as the class starts - Hobby accounts are always free If they\u0026rsquo;d like to explore Pro features, we\u0026rsquo;ve set up \u0026ldquo;Stanford CS\u0026rdquo; as a category in the Vercel Credits for Startups application page. Each team will receive $1,200 in credits. For \u0026ldquo;Proof of Partnership\u0026rdquo;, they should upload a screenshot of the syllabus / an e-mail from Jay about the course. A 14-day Pro Trial begins (they can email josh.oynick@vercel.com with any issues). At the end of the trial, if the teams choose to remain on Pro and add credit card info, the credits will begin to draw down. If teams add payment info, they are strongly encouraged to set hard spending limits right away (documentation). Tools of the Trade # Documentation # Github Wiki Here are some examples Project Management # Github issues, milestones, labels, comments Here is an example Real-time Team Communication # Slack, Messenger Discovery, Concepting Phase # Hand draw, Miro, Figma, GSlides Build Phase # Github Code (Pull request methodology) Github issues (code focused) Leverage Copilot, chatGPT, Bard as is useful GitHub Resources # Make sure your git activity is attributed to you Github Cheat Sheet - Page 1 Github Cheat Sheet - Page 2 How to make a Github pull request Miscellany Resources # Messaging Activation Matrix Beginner\u0026rsquo;s resources for Building Web and mobile apps CS210 Recommended Reads for Aspiring Program Managers/Product Leaders Contract of Deliverables Template Product Requirements Document (PRD) An incomplete list of ethics questions to consider Team Budget Guidelines Grading Rubric Make an animated gif from screen shots Digital sticky note option "},{"id":3,"href":"/docs/vr/getting-started-in-vr/","title":"Getting Started in VR","section":"AR/VR","content":" Getting Started in VR # Original document by Mindy:\nLast year I did a VR project in CS210 with Unreal Engine, and thought that I would share some thoughts and tips with you guys to get you started.\nDemos # Title Link Description Oculus Demos repository https://share.oculus.com/ The definitive VR experiences and demos repository from Oculus. Elite Dangerous https://www.elitedangerous.com/ A space exploration video game with a great VR experience. Costs $45. Things to think about # what sort of approaches to games/experiences might work for your project? What are some common/different control interfaces/menus/UI that the demos use? What works better or worse? what hasn\u0026rsquo;t been done/done well? What can you bring to the table? There are also loads of other VR demos available - if you want to see if a kind of demo exists just Google it and you\u0026rsquo;ll probably turn something up. Also check out Cardboard demos on Google Play (I recommend VRSE) to see what sort of things are happening in the non-gaming/low-end space. VR best practices # Google - http://www.fastcodesign.com/3053288/3-tips-on-designing-for-vr-from-google, https://play.google.com/store/apps/details?id=com.google.vr.cardboard.apps.designlab Unreal - https://docs.unrealengine.com/latest/INT/Platforms/VR/ContentSetup/index.html Oculus - https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/ Search for \u0026ldquo;VR best practices for [insert company/device/controller]\u0026rdquo; and you\u0026rsquo;ll probably unearth a ton of stuff Unreal Engine tutorials # Unreal has a really good YouTube series where a developer sits there and shows you how to make a demo. This was by far the most helpful way to get me oriented. https://www.youtube.com/playlist?list=PLZlv_N0_O1gak1_FoAJVrEGiLIploeF3F LayoutVR has good walkthrough guides You can go read the official Unreal Engine guides too Unreal has a steep learning curve if you\u0026rsquo;ve never done game dev before, so make sure you allocate a lot of time to learning it! Controllers # This is still a huge pain point in VR, and likely the bottleneck to what you can accomplish. Oculus Touch is probably your best bet if you can get your hands on it Razer Hydra/Sixense STEM or the Kinect are other good But really, try to find all your possible options. If the controller is not exactly commercially available yet, don\u0026rsquo;t be afraid to ask for a prototype. You won\u0026rsquo;t believe how willing people are to work with you once you drop the Stanford/Unreal/Facebook/Oculus name. Mindy\u0026rsquo;s team\u0026rsquo;s work from last year # https://unrealvr.wordpress.com/ I\u0026rsquo;ve also done stuff on the Cardboard and some product research on the space in general. Feel free to ask me any questions! There is a LOT of information to just absorb if you\u0026rsquo;re completely new to the space.\nFeeling a bit overwhelmed is normal, just be tenacious, ask tons of questions and keep on reading!\n"},{"id":4,"href":"/docs/web/frontendessentials/","title":"Frontend Essentials","section":"Web","content":" Frontend Development Guide for Next.js # Component Architecture # Understanding Components # Components are the building blocks of your React application. They should be:\nSmall and focused on a single responsibility Reusable across different parts of your application Easy to test and maintain Here\u0026rsquo;s an example of a well-structured component:\n// components/ui/Card.tsx // Define the shape of props(parameters) that this component accepts interface CardProps { title: string // Required: The card\u0026#39;s title children: React.ReactNode // Required: The content inside the card className?: string // Optional: Additional CSS classes } // Card component that renders a styled container with a title and content export function Card({ title, children, className = \u0026#39;\u0026#39; // Default to empty string if no className provided }: CardProps) { return ( // Main container with base styles and optional custom classes \u0026lt;div className={` rounded-lg /* Rounded corners */ shadow-md /* Subtle shadow for depth */ p-4 /* Padding on all sides */ ${className} /* Custom classes passed as prop */ `}\u0026gt; {/* Title section with consistent styling */} \u0026lt;h2 className=\u0026#34;text-xl font-bold mb-2\u0026#34;\u0026gt;{title}\u0026lt;/h2\u0026gt; {/* Content section that renders the children */} \u0026lt;div\u0026gt;{children}\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ) } This Card component demonstrates several best practices:\nClear interface definition Props with TypeScript types Default values for optional props Composition using children prop Flexible styling with className prop Component Organization # Organize your components into categories:\nui/: Basic building blocks (buttons, cards, inputs) features/: Business-specific components (UserProfile, OrderForm) layout/: Page structure components (Header, Footer, Sidebar) Page Structure and Routing # Next.js 13+ uses the App Router, which provides:\nFile-based routing Nested layouts Server and Client Components Loading and error states Here\u0026rsquo;s a basic page structure:\n// app/posts/page.tsx import { Suspense } from \u0026#39;react\u0026#39; import { PostList } from \u0026#39;@/components/features/PostList\u0026#39; import { LoadingSpinner } from \u0026#39;@/components/ui/LoadingSpinner\u0026#39; // Posts page component - demonstrates Next.js 13+ app directory structure export default function PostsPage() { return ( // Container with responsive padding and max-width \u0026lt;div className=\u0026#34; container /* Set max-width and center content */ mx-auto /* Center horizontally */ px-4 /* Horizontal padding */ py-8 /* Vertical padding */ \u0026#34;\u0026gt; {/* Page title with consistent styling */} \u0026lt;h1 className=\u0026#34; text-3xl /* Large text size */ font-bold /* Bold weight */ mb-6 /* Bottom margin */ \u0026#34;\u0026gt; Posts \u0026lt;/h1\u0026gt; {/* Suspense boundary for loading state - Shows LoadingSpinner while PostList is loading - Part of React\u0026#39;s concurrent features */} \u0026lt;Suspense fallback={\u0026lt;LoadingSpinner /\u0026gt;}\u0026gt; \u0026lt;PostList /\u0026gt; \u0026lt;/Suspense\u0026gt; \u0026lt;/div\u0026gt; ) } Key concepts to remember:\nUse Suspense for loading states Keep pages simple and focused on layout Move complex logic to components State Management # Choose the right state management approach based on your needs:\nLocal State (useState):\nFor component-specific state When state doesn\u0026rsquo;t need to be shared Context API:\nFor state shared across many components Theme, user preferences, authentication state Here\u0026rsquo;s a practical theme context example:\n// contexts/ThemeContext.tsx import { createContext, useContext, useState } from \u0026#39;react\u0026#39; // Define the shape of our context state and methods interface ThemeContextType { theme: \u0026#39;light\u0026#39; | \u0026#39;dark\u0026#39; // Current theme toggleTheme: () =\u0026gt; void // Function to switch themes } // Create the context with undefined as initial value // We use undefined initially so TypeScript can warn us if we forget the Provider const ThemeContext = createContext\u0026lt;ThemeContextType | undefined\u0026gt;(undefined) // Provider component that wraps our app and makes theme available to any child component export function ThemeProvider({ children }: { children: React.ReactNode // Components that will have access to the theme }) { // Manage the theme state const [theme, setTheme] = useState\u0026lt;\u0026#39;light\u0026#39; | \u0026#39;dark\u0026#39;\u0026gt;(\u0026#39;light\u0026#39;) // Function to toggle between light and dark themes const toggleTheme = () =\u0026gt; setTheme(prev =\u0026gt; prev === \u0026#39;light\u0026#39; ? \u0026#39;dark\u0026#39; : \u0026#39;light\u0026#39;) // Provide the theme context to children return ( \u0026lt;ThemeContext.Provider value={{ theme, // Current theme state toggleTheme // Function to change theme }} \u0026gt; {children} \u0026lt;/ThemeContext.Provider\u0026gt; ) } // Custom hook to use the theme context export function useTheme() { // Get the context value const context = useContext(ThemeContext) // Throw an error if used outside of provider if (context === undefined) { throw new Error(\u0026#39;useTheme must be used within a ThemeProvider\u0026#39;) } return context } /* Usage example: function MyComponent() { const { theme, toggleTheme } = useTheme() return ( \u0026lt;button onClick={toggleTheme}\u0026gt; Current theme: {theme} \u0026lt;/button\u0026gt; ) } */ Forms and User Input # Forms are a crucial part of web applications. Important considerations:\nInput validation Error handling Accessibility User feedback Type safety Use a custom form hook for consistent form handling:\n// hooks/useForm.ts import { useState, ChangeEvent, FormEvent } from \u0026#39;react\u0026#39; // Define the options that our hook accepts interface UseFormOptions\u0026lt;T\u0026gt; { initialValues: T // Initial form values onSubmit: (values: T) =\u0026gt; Promise\u0026lt;void\u0026gt; // Submit handler validate?: (values: T) =\u0026gt; Partial\u0026lt;Record\u0026lt;keyof T, string\u0026gt;\u0026gt; // Optional validation } // Custom hook for form handling with TypeScript generics export function useForm\u0026lt;T extends Record\u0026lt;string, any\u0026gt;\u0026gt;({ initialValues, onSubmit, validate }: UseFormOptions\u0026lt;T\u0026gt;) { // State management const [values, setValues] = useState\u0026lt;T\u0026gt;(initialValues) // Form values const [errors, setErrors] = useState\u0026lt;Partial\u0026lt;Record\u0026lt;keyof T, string\u0026gt;\u0026gt;\u0026gt;({}) // Validation errors const [submitting, setSubmitting] = useState(false) // Submit state // Handle input changes const handleChange = (e: ChangeEvent\u0026lt;HTMLInputElement | HTMLTextAreaElement\u0026gt;) =\u0026gt; { const { name, value } = e.target setValues(prev =\u0026gt; ({ ...prev, [name]: value })) } // Handle form submission const handleSubmit = async (e: FormEvent) =\u0026gt; { e.preventDefault() // Validate if validation function is provided if (validate) { const validationErrors = validate(values) if (Object.keys(validationErrors).length \u0026gt; 0) { setErrors(validationErrors) return } } // Submit the form setSubmitting(true) try { await onSubmit(values) } finally { setSubmitting(false) } } return { values, // Current form values errors, // Validation errors submitting, // Whether form is submitting handleChange, // Input change handler handleSubmit // Form submit handler } } /* Usage example: const form = useForm({ initialValues: { email: \u0026#39;\u0026#39;, password: \u0026#39;\u0026#39; }, validate: (values) =\u0026gt; { const errors: Record\u0026lt;string, string\u0026gt; = {} if (!values.email) errors.email = \u0026#39;Required\u0026#39; if (!values.password) errors.password = \u0026#39;Required\u0026#39; return errors }, onSubmit: async (values) =\u0026gt; { await submitToAPI(values) } }) */ Best Practices # Component Design: # Keep components focused and small Use TypeScript interfaces for props Handle loading and error states Make components reusable State Management: # Keep state close to where it\u0026rsquo;s used Use context sparingly Handle loading/error states "},{"id":5,"href":"/docs/ml/extending-llms/","title":"Extending LLMs","section":"ML Engineering","content":" Extending LLMs # Below are some ways to go beyond just LLMs and use your custom data. These include methods such as Retrieval Augmented Generation (RAG), finetuning, and even searching over tabular data.\nRetrieval Augmented Generation # Language models are typically pretrained on vast amounts of data, but they may require updated information or data tailored to a specific use case. Retraining a language model on a large amount of data might prove to be expensive. In many scenarios, extending large language models with external knowledge bases becomes essential to provide relevant and accurate outputs. This is where Retrieval-Augmented Generation (RAG) comes into play. RAG consists of two key components: a retrieval system to identify relevant items or contextual information and a mechanism to integrate that information into the model\u0026rsquo;s output, often through prompting.\nTo implement a knowledge base for RAG, you often use an embedding model, which encodes information into a lower-dimensional space, and a vector database, which performs similarity searches within that space. Tools like Chroma, Pinecone, and pgvector are popular options for this purpose. These systems enable efficient retrieval of contextually relevant data to enhance the model\u0026rsquo;s responses. Below is a simple code example demonstrating this process, and you can find several excellent tutorials for further exploration.\n# Make sure to pip install chromadb openai import chromadb from chromadb.utils import embedding_functions from chromadb.config import Settings # Step 1: Initialize the Chroma client chroma_client = chromadb.Client( Settings( persist_directory=\u0026#34;./chroma_db\u0026#34;, # Path to store the database chroma_db_impl=\u0026#34;duckdb+parquet\u0026#34;, # Database backend ) ) # Step 2: Define the embedding function (OpenAI in this case) openai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=\u0026#34;your_openai_api_key\u0026#34;, # Replace with your OpenAI API key model_name=\u0026#34;text-embedding-ada-002\u0026#34;, ) # Step 3: Create or get a collection collection = chroma_client.get_or_create_collection( name=\u0026#34;knowledge_base\u0026#34;, embedding_function=openai_ef ) # Step 4: Add data to the collection documents = [ \u0026#34;The Eiffel Tower is located in Paris, France.\u0026#34;, \u0026#34;The Great Wall of China is one of the Seven Wonders of the World.\u0026#34;, \u0026#34;Python is a popular programming language for machine learning.\u0026#34;, \u0026#34;The Amazon Rainforest is the largest rainforest in the world.\u0026#34;, \u0026#34;Albert Einstein developed the theory of relativity.\u0026#34; ] doc_ids = [f\u0026#34;doc_{i}\u0026#34; for i in range(len(documents))] collection.add( documents=documents, ids=doc_ids ) # Step 5: Query the collection with a similar question query = \u0026#34;Who created the theory of relativity?\u0026#34; results = collection.query( query_texts=[query], n_results=3 # Return top 3 most similar results ) # Step 6: Display results print(\u0026#34;Query:\u0026#34;, query) print(\u0026#34;Most similar documents:\u0026#34;) for i, doc in enumerate(results[\u0026#34;documents\u0026#34;][0]): print(f\u0026#34;{i + 1}. {doc}\u0026#34;) Using these documents, we can then use them when prompting a final language model to improve overall responses. Finding the best way to prompt a model is also known as prompt engineering, and there are several popular methods, including chain-of-thought (including the model\u0026rsquo;s reasoning step-by-step) as well as few-shot prompting (providing a few sample examples).\nFinetuning Language Models # Sometimes, prompting isn\u0026rsquo;t enough. You may want to adjust and steer the tone of a large language model (LLM) or train it on additional examples that exceed the limits of a single prompt. To achieve this, fine-tuning is a powerful tool.\nFine-tuning is the process of adapting a pre-trained model for specific tasks or use cases. It is significantly easier and more cost-effective to improve a base model through fine-tuning than to train a model from scratch.\nDuring rapid prototyping, you\u0026rsquo;ll likely rely on a pre-trained LLM. If needed, there are well-documented ways to fine-tune pre-trained LLMs (OpenAI Fine-Tuning Guide). The most important steps in fine-tuning include:\nPreparing your data: Ensure your dataset is in the correct format, and include both positive and negative examples so the LLM can learn to handle a variety of scenarios effectively. Choosing the right dataset size: A minimum of 10 examples can be sufficient, but you’ll observe clearer improvements with 50–100 examples or more. Setting up evaluations: Create a set of prompts that can be used to test and measure the fine-tuned model’s performance against your desired outcomes. Once the data and evaluations are ready, you can initiate a fine-tuning job using OpenAI\u0026rsquo;s API:\nfrom openai import OpenAI client = OpenAI() job = client.fine_tuning.jobs.create( training_file=\u0026#34;file-all-about-the-weather\u0026#34;, model=\u0026#34;gpt-4o-2024-08-06\u0026#34;, method={ \u0026#34;type\u0026#34;: \u0026#34;dpo\u0026#34;, \u0026#34;dpo\u0026#34;: { \u0026#34;hyperparameters\u0026#34;: {\u0026#34;beta\u0026#34;: 0.1}, }, }, ) After fine-tuning, you can use the customized model in your application:\nfrom openai import OpenAI client = OpenAI() completion = client.chat.completions.create( model=\u0026#34;ft:gpt-4o-mini:my-org:custom_suffix:id\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello!\u0026#34;} ] ) print(completion.choices[0].message) You can experiment with different hyperparameters to improve the performance of your fine-tuned model. Fine-tuning an open-source model is also a viable option. There are excellent resources available, such as the Llama Fine-Tuning Guide. Additionally, there are many techniques (IBM Fine-Tuning Techniques) to make the process more cost-effective and computationally efficient.\nTool Use # "},{"id":6,"href":"/docs/web/databases/","title":"Databases","section":"Web","content":" Databases # Most modern web applications often require a database to store and manage data. This guide will lead you through setting up a database for a Next.js project, using hosted database services, and performing basic CRUD (Create, Read, Update, Delete) operations with Prisma, a popular ORM (Object Relational Mapper).\nChoosing a Database # SQL (PostgreSQL via Supabase) # Choose when you need: Data relationships, ACID compliance, complex queries Free tier available, excellent documentation Built-in row-level security and authentication NoSQL (MongoDB Atlas) # Choose when you need: Flexible schema, high write throughput, horizontal scaling Free tier available, great for prototypes Native JSON support Setup Guide for Hosted Databases # PostgreSQL via Supabase # Sign up here Create project Get connection string from: Settings -\u0026gt; Database MongoDB Atlas # Sign up here Create cluster (M0 Free tier) Get connection string from: Connect -\u0026gt; Connect your application Adding Prisma to Your Next.js Project # Prisma makes working with databases in Next.js efficient and type-safe. Follow these steps:\nInstall Prisma by running the following commands in your project:\nnpm install prisma --save-dev npm install @prisma/client Initialize Prisma\nnpx prisma init This creates a prisma/ directory with a schema.prisma file.\nUpdate the schema.prisma file by replacing the datasource block with your connection string. For example:\ndatasource db { provider = \u0026#34;postgresql\u0026#34; // Change to \u0026#34;mongodb\u0026#34; for MongoDB url = env(\u0026#34;DATABASE_URL\u0026#34;) } Add a simple model:\nmodel Post { id Int @id @default(autoincrement()) title String content String createdAt DateTime @default(now()) } Move the model to your database:\nnpx prisma migrate dev --name description_of_changes or for quick prototyping and no version control:\nnpx prisma db push This creates the Post table in your database.\nCRUD Operations in Next.js # Here’s how to perform basic CRUD operations using Prisma in API routes:\na. Create a Post # Create a new file app/api/posts/create.js:\nimport { PrismaClient } from \u0026#34;@prisma/client\u0026#34;; const prisma = new PrismaClient(); export default async function handler(req, res) { if (req.method === \u0026#34;POST\u0026#34;) { const { title, content } = req.body; const post = await prisma.post.create({ data: { title, content, }, }); res.status(200).json(post); } else { res.status(405).json({ message: \u0026#34;Method not allowed\u0026#34; }); } } b. Read Posts # Create a new file app/api/posts/index.js:\nimport { PrismaClient } from \u0026#34;@prisma/client\u0026#34;; const prisma = new PrismaClient(); export default async function handler(req, res) { if (req.method === \u0026#34;GET\u0026#34;) { const posts = await prisma.post.findMany(); res.status(200).json(posts); } else { res.status(405).json({ message: \u0026#34;Method not allowed\u0026#34; }); } } c. Update a Post # Create a new file app/api/posts/update.js:\nimport { PrismaClient } from \u0026#34;@prisma/client\u0026#34;; const prisma = new PrismaClient(); export default async function handler(req, res) { if (req.method === \u0026#34;PUT\u0026#34;) { const { id, title, content } = req.body; const post = await prisma.post.update({ where: { id: Number(id) }, data: { title, content }, }); res.status(200).json(post); } else { res.status(405).json({ message: \u0026#34;Method not allowed\u0026#34; }); } } d. Delete a Post # Create a new file app/api/posts/delete.js:\nimport { PrismaClient } from \u0026#34;@prisma/client\u0026#34;; const prisma = new PrismaClient(); export default async function handler(req, res) { if (req.method === \u0026#34;DELETE\u0026#34;) { const { id } = req.body; await prisma.post.delete({ where: { id: Number(id) }, }); res.status(200).json({ message: \u0026#34;Post deleted\u0026#34; }); } else { res.status(405).json({ message: \u0026#34;Method not allowed\u0026#34; }); } } You have now created RESTful API Endpoints for your database operations!\nEnvironment Variables # Store your database connection string securely in a .env file:\nDATABASE_URL=your-database-connection-string Ensure .env is added to your .gitignore to prevent it from being committed.\nPerformance Tips # Add indexes for frequently queried fields Use select to limit returned fields Consider caching for read-heavy operations "},{"id":7,"href":"/docs/ml/general-advice/","title":"General Advice","section":"ML Engineering","content":" General Advice # Below are some general tips, aggregated from industry experts and 210 alums.\nBuy first, not Build. # For initial functional prototyping and for most of the former part of your application\u0026rsquo;s lifecycle, lean on existing APIs to do a lot of the heavy lifting. These models are already quite powerful and are able to handle almost all of the requests you send it. If certain users or design partners necessitate even more advanced functionality, though, consider working and finetuning your own LLMs.\nWhen unsure, lean on the literature. # Oftentimes, issues with performance lie in the technical reports.\n"},{"id":8,"href":"/docs/ml/basic/","title":"Basic ML","section":"ML Engineering","content":" Basic ML # Below are some snippets for common ML use cases such as NLP \u0026ndash; namely, LLMs \u0026ndash; computer vision, and even standard statistical ML.\nLarge Language Models (LLMs) # For initial iteration, using LLM APIs is probably the best way to go. There are several models out there, each with their own strengths. Below is an example using the OpenAI API to make a call to a language model:\nfrom openai import OpenAI client = OpenAI() completion = client.chat.completions.create( model=\u0026#34;gpt-4o-mini\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34;}, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Write a haiku about recursion in programming.\u0026#34; } ] ) print(completion.choices[0].message) As you continue to iterate, you may want to leverage the power of open-source models. For this, Hugging Face \u0026ndash; in particular, the SentenceTransformers module \u0026ndash; will be your best friend. Just specify your language model and you\u0026rsquo;ll be good to go:\nfrom sentence_transformers import SentenceTransformer model = SentenceTransformer(\u0026#39;paraphrase-MiniLM-L6-v2\u0026#39;) # Sentences we want to encode. Example: sentence = [\u0026#39;This framework generates embeddings for each input sentence\u0026#39;] # Sentences are encoded by calling model.encode() embedding = model.encode(sentence) Of course, utilizing an open-source model will also mean figuring out how to serve the model. There are a plethora of tools for this, including Modal, Baseten, Together AI, and many others.\nComputer Vision # Similar to LLMs, there are several Vision APIs available that can be used for quick experimentation. All of the larger cloud platforms have existing offerings \u0026ndash; namely GCP and Azure \u0026ndash; but it\u0026rsquo;s also important to note that many of the SOTA models are already multimodal. For instance, GPT-4V already has strong vision capabilities:\nfrom openai import OpenAI client = OpenAI() response = client.chat.completions.create( model=\u0026#34;gpt-4o-mini\u0026#34;, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;What\u0026#39;s in this image?\u0026#34;}, { \u0026#34;type\u0026#34;: \u0026#34;image_url\u0026#34;, \u0026#34;image_url\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\u0026#34;, }, }, ], } ], max_tokens=300, ) print(response.choices[0]) For using custom models, torchvision \u0026ndash; and PyTorch more broadly \u0026ndash; are quite helpful. These packages allow us to both build our own models from scratch as well as use existing large pre-trained models. we can both An often common use case is to take a pre-trained image classification or segmentation model and further finetuning it for a specific task. This is also known as transfer learning, and there are several tutorials out there for the task.\nStatistical ML # Finally, for simple problems, statistical techniques may be sufficient. sklearn is often what\u0026rsquo;s used in industry for these tasks, and it can support tasks ranging from nearest neighbors clustering to gradient boosting. Here\u0026rsquo;s an example of using sklearn for linear regression:\nfrom sklearn.linear_model import LinearRegression import numpy as np # Create some sample data X = np.array([[1], [2], [3], [4], [5]]) # Features y = np.array([2, 4, 5, 4, 5]) # Target values # Create and fit the model model = LinearRegression() model.fit(X, y) # Make predictions predictions = model.predict(X) # Get model coefficients and intercept print(f\u0026#34;Slope: {model.coef_[0]:.2f}\u0026#34;) print(f\u0026#34;Intercept: {model.intercept_:.2f}\u0026#34;) # Calculate R-squared score r_squared = model.score(X, y) print(f\u0026#34;R-squared: {r_squared:.2f}\u0026#34;) "}]